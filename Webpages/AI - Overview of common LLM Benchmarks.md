
[AI: Overview of common LLM benchmarks](https://dev.to/hmcodes/ai-an-overview-of-common-llm-benchmarks-3i7b)


## Ten Common Benchmarks

1. [[AI2 Reasoning Challenge (ARC)]]
2. [[HellaSwag (Harder Endings, Longer contexts and Low-shot Activities for Situations With Adversarial Generations)]]
3. [[MMLU (Massive Multitask Language Understanding)]]
4. [[GSM8K (Grade School Math 8K)]]
5. [[TruthfulQA]]
6. [[Winogrande]]
7. [[Chatbot Arena]]
8. [[HumanEval]] 
9.  [[MBPP (Mostly Basic Programming Problems)]]
10. [[GLUE (General Language Understanding Evaluation)]]

