From: [[AI - Overview of common LLM Benchmarks]]

Chain of Thought (CoT) is a technique used in language models where the model is encouraged to generate a series of intermediate reasoning steps before arriving at the final answer. It mimics how humans often solve complex problems by breaking them down into smaller, manageable steps.

### Example:
	Question:
	"Sarah has 7 apples. 
	She gives 3 apples to her friend and then buys 5 more. 
	How many apples does Sarah have now?"
	
	Model's Response:
	
	"Sarah starts with 7 apples."
	"She gives 3 apples away, so she has 7 - 3 = 4 apples left."
	"She buys 5 more apples, so now she has 4 + 5 = 9 apples."
	Answer: "Sarah has 9 apples."

## Resources
[IBM - Chain of Thought](https://www.ibm.com/topics/chain-of-thoughts#f01) 


## Papers 
#papers_to_process 

Boshi Wang, S. M. (2022). Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters. 2717-2739, https://doi.org/10.48550/arXiv.2212.10001.

Jason Wei, X. W. (2022). Chain of Thought Prompting Elicits Reasoning in Large Language Models. 36th Conference on Neural Information Processing Systems (NeurIPS 2022).

Zheng Chu, J. C. (2023). A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future. ArXiv, abs/2309.15402.

Omar Shaikh, H. Z. (2022, December). On Second Thought, Letâ€™s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning. ArXiv, abs/2212.08061. https://doi.org/10.48550/arXiv.2212.08061.

Zhuosheng Zhang, A. Z. (2022). Automatic Chain of Thought Prompting in Large Language Models. ArXiv, abs/2210.03493. https://doi.org/10.48550/arXiv.2210.03493.

Zhuosheng Zhang, A. Z. (2023). Multimodal Chain-of-Thought Reasoning in Language Models. ArXiv, abs/2302.00923. https://doi.org/10.48550/arXiv.2302.00923.

Yao, Z. L. (2023). Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models. ArXiv, abs/2305.16582. https://doi.org/10.48550/arXiv.2305.16582.

Kashun Shum, S. D. (2023). Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data. ArXiv, abs/2302.12822. https://doi.org/10.48550/arXiv.2302.12822.

A Vaswani, N. S. (2017). Attention is all you need. Advances in neural information processing systems.

Zhengyan Zhang, Y. G. (2021). CPM-2: Large-scale Cost-effective Pre-trained Language Models. AI Open, 2, 216--224.

L Zheng, N. G. (2021). When does pretraining help? assessing self-supervised learning for law and the casehold dataset of 53,000+ legal holdings. In Proceedings of the eighteenth international conference on artificial intelligence and law , 159-168.

S Roller, E. D. (2020). Recipes for building an open-domain chatbot. arXiv preprint arXiv:2004.13637 .
