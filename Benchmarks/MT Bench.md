Paper: [[Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena]]

## From [[An In-depth Guide to Benchmarking LLMs]]
Multi-turn dialogs

**human evaluation** through crowdsourced platform

crowd-sourced platform that allows users to evaluate a variety of chatbots by entering a prompt and comparing the two responses side-by-side. Users vote on best response

eight main types of user prompts: 
* writing, 
* roleplay, 
* extraction, 
* reasoning, 
* math, 
* coding, 
* knowledge I (STEM), 
* knowledge II (humanities)

160 questions.

#### Pros
* Measures a modelâ€™s ability to answer subsequent, related questions
#### Cons
* Though carefully curated, the dataset is small
* Hard to simulate the broad and unpredictable nature of conversations
