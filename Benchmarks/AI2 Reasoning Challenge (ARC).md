Paper: [[Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge]]

## From [[AI - Overview of common LLM Benchmarks]]

**The goal**: To measure a model's ability to correctly answer questions and demonstrate sound reasoning.

 **The test**: To correctly answer more than 7000 grade-school science questions.

## From [[An In-depth Guide to Benchmarking LLMs]]

 7787 **four-option multiple-choice** science questions that range from a 3rd to 9th-grade difficulty level.

More comprehensive and difficult benchmark than previous QA benchmarks:
+ [[Stanford Question and Answer Dataset (SQuAD)]]
+ [[Stanford Natural Language Inference (SNLI)]] corpus, 

These only measured a model’s ability to extract the correct answer from a passage.

#### Pros
+ Varied and challenging dataset
+ Pushes AI vendors to improve QA abilities – not just through fact retrieval but by integrating information from several sentences.  
#### Cons
+ Only consists of scientific questions 
