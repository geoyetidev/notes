Paper: [[Training Verifiers to Solve Math Word Problems]]

## From [[AI - Overview of common LLM Benchmarks]]
**The goal**: To evaluate a modelâ€™s capability in solving grade-school-level math problems.

**The test**: The model is presented with more than 8,000 grade-school math word problems and must solve them correctly.

## From [[An In-depth Guide to Benchmarking LLMs]]

 Multi-step mathematical reasoning abilities. 
 
 8,500 grade-school-level math word problems devised by humans, which is divided into 7,500 training problems and 1,00 test problems. 

The solution to each problem is collected in natural language form as opposed to a mathematical expression. 

**does not describe how evaluation is done** 

#### Pros 
* Mathematical reasoning thus reveals a critical weakness in modern language models.
* Problems are framed with high linguistic diversity 
#### Cons
* Problems are relatively simple to solve, so the benchmark could be obsolete fairly soon 
