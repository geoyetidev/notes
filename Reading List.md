## Websites
- [x] [[AI - Overview of common LLM Benchmarks]]
- [x] [[An In-depth Guide to Benchmarking LLMs]]
- [ ] [[Benchmarking AI]]
- [ ] [[AI Fairness 360]]
- [ ] [[Tensorflow - Fairness Indicators]]
- [ ] [[DataComp]]
- [ ] [[DataPerf]]
- [ ] [[An introduction to code LLM benchmarks for software engineers]]
- [ ] [[Chain of Thought]]
- [ ] [[Errors in the MMLU- The Deep Learning Benchmark is Wrong Surprisingly Often]]
## Benchmark Papers
- [ ] [[Chatbot Arena- An Open Platform for Evaluating LLMs by Human Preference]]
- [ ] [[Evaluating Large Language Models Trained on Code]]
- [ ] [[GLUE- A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding]]
- [ ] [[HellaSwag- Can a Machine Really Finish Your Sentence?]]
- [ ] [[Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena]]
- [ ] [[Measuring Massive Multitask Language Understanding]]
- [ ] [[Program Synthesis with Large Language Models]]
- [ ] [[SuperGLUE- A Stickier Benchmark for General-Purpose Language Understanding Systems]]
- [ ] [[Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge]]
- [ ] [[Training Verifiers to Solve Math Word Problems]]
- [ ] [[TruthfulQA- Measuring How Models Mimic Human Falsehoods]]
- [ ] [[WinoGrande- An Adversarial Winograd Schema Challenge at Scale]]
## Other Papers
- [ ] [[NIST - AI RMF]]
- [ ] [[Towards Ecologically Valid Research on Language User Interfaces]]
